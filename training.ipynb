{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:36:47.501763: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 16:36:47.765780: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 16:36:47.765927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 16:36:47.804723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 16:36:47.894077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 16:36:48.641564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SCNet model using tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPool1D, Flatten, Dense, Dropout, BatchNormalization, AvgPool1D, ReLU, Softmax\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D, GlobalAvgPool1D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class TransposeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, perm=None, **kwargs):\n",
    "        super(TransposeLayer, self).__init__(**kwargs)\n",
    "        self.perm = perm\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.transpose(inputs, perm=self.perm)\n",
    "\n",
    "def mffm_block(input):\n",
    "    # Convolutional layer with ReLU activation\n",
    "    x1 = Conv1D(filters=8, kernel_size=5, padding='same')(input)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = ReLU()(x1)\n",
    "\n",
    "    # Concatenate input and x1 along the channel axis\n",
    "    x2 = Concatenate(axis=-1)([input, x1])\n",
    "    x2 = Conv1D(filters=16, kernel_size=5, padding='same')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = ReLU()(x2)\n",
    "\n",
    "    # Concatenate input, x1, and x2 along the channel axis\n",
    "    x3 = Concatenate(axis=-1)([input, x1, x2])\n",
    "    return x3\n",
    "\n",
    "\n",
    "def scnet(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    # Tranpose input\n",
    "    x = TransposeLayer(perm=[0, 2, 1])(input)\n",
    "    # Implementing SILM\n",
    "    gap = GlobalAvgPool1D(data_format='channels_first',keepdims=True)(x)\n",
    "    gmp = GlobalMaxPool1D(data_format='channels_first',keepdims=True)(x)\n",
    "    gsp = tf.expand_dims(tf.sqrt(tf.reduce_sum((x - gap) ** 2, axis=2)) / 21, 2)\n",
    "    # Apply dropout on each\n",
    "    gap = Dropout(0.05)(gap)\n",
    "    gmp = Dropout(0.05)(gmp)\n",
    "    gsp = Dropout(0.05)(gsp)\n",
    "    # Concatenate with input\n",
    "    x = Concatenate(axis=2)([x, gap, gmp, gsp])\n",
    "\n",
    "    # Take avg and maxpool\n",
    "    x1 = MaxPool1D(3, strides = 3)(x)\n",
    "    x2 = AvgPool1D(3, strides = 3)(x)\n",
    "    x = Concatenate(axis=1)([x1, x2])\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x1 = mffm_block(x)\n",
    "    x2 = mffm_block(x)\n",
    "    x = x1 + x2\n",
    "    \n",
    "    # Apply spatial dropout\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPool1D(2, strides = 2)(x)\n",
    "    x = Conv1D(32, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x1 = mffm_block(x)\n",
    "    x2 = mffm_block(x)\n",
    "    x = x1 + x2\n",
    "\n",
    "    x = Conv1D(32, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = mffm_block(x)\n",
    "    x = MaxPool1D(2, strides = 2)(x)\n",
    "    x = Conv1D(32, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Apply gap\n",
    "    x = GlobalAvgPool1D(data_format='channels_first',keepdims=True)(x)\n",
    "\n",
    "    # Flatten the output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2)(x)\n",
    "\n",
    "    # Apply softmax\n",
    "    outputs = Softmax()(x)\n",
    "    return Model(inputs=input, outputs=outputs)\n",
    "\n",
    "\n",
    "model = scnet((21, 42001))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 21, 42001)]          0         []                            \n",
      "                                                                                                  \n",
      " transpose_layer (Transpose  (None, 42001, 21)            0         ['input_1[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 42001, 1)             0         ['transpose_layer[0][0]']     \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLamb  (None, 42001, 21)            0         ['transpose_layer[0][0]',     \n",
      " da)                                                                 'global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.math.pow (TFOpLambda)    (None, 42001, 21)            0         ['tf.math.subtract[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLa  (None, 42001)                0         ['tf.math.pow[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.sqrt (TFOpLambda)   (None, 42001)                0         ['tf.math.reduce_sum[0][0]']  \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambd  (None, 42001)                0         ['tf.math.sqrt[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 42001, 1)             0         ['transpose_layer[0][0]']     \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 42001, 1)             0         ['tf.math.truediv[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 42001, 1)             0         ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 42001, 1)             0         ['global_max_pooling1d[0][0]']\n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 42001, 1)             0         ['tf.expand_dims[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 42001, 24)            0         ['transpose_layer[0][0]',     \n",
      "                                                                     'dropout[0][0]',             \n",
      "                                                                     'dropout_1[0][0]',           \n",
      "                                                                     'dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 14000, 24)            0         ['concatenate[0][0]']         \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d (Average  (None, 14000, 24)            0         ['concatenate[0][0]']         \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 28000, 24)            0         ['max_pooling1d[0][0]',       \n",
      " )                                                                   'average_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 28000, 24)            96        ['concatenate_1[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 28000, 8)             968       ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 28000, 8)             968       ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 28000, 8)             32        ['conv1d[0][0]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 28000, 8)             32        ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 28000, 8)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 28000, 8)             0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 28000, 32)            0         ['batch_normalization[0][0]', \n",
      " )                                                                   're_lu[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 28000, 32)            0         ['batch_normalization[0][0]', \n",
      " )                                                                   're_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 28000, 16)            2576      ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 28000, 16)            2576      ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 28000, 16)            64        ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 28000, 16)            64        ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 28000, 16)            0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 28000, 16)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 28000, 48)            0         ['batch_normalization[0][0]', \n",
      " )                                                                   're_lu[0][0]',               \n",
      "                                                                     're_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 28000, 48)            0         ['batch_normalization[0][0]', \n",
      " )                                                                   're_lu_2[0][0]',             \n",
      "                                                                     're_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 28000, 48)            0         ['concatenate_3[0][0]',       \n",
      " Lambda)                                                             'concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 28000, 48)            0         ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 14000, 48)            0         ['dropout_3[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 13998, 32)            4640      ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 13998, 32)            128       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 13998, 32)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 13998, 8)             1288      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 13998, 8)             1288      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 13998, 8)             32        ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 13998, 8)             32        ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 13998, 8)             0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 13998, 8)             0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 13998, 40)            0         ['re_lu_4[0][0]',             \n",
      " )                                                                   're_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 13998, 40)            0         ['re_lu_4[0][0]',             \n",
      " )                                                                   're_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 13998, 16)            3216      ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 13998, 16)            3216      ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 13998, 16)            64        ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 13998, 16)            64        ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 13998, 16)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 13998, 16)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 13998, 56)            0         ['re_lu_4[0][0]',             \n",
      " )                                                                   're_lu_5[0][0]',             \n",
      "                                                                     're_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 13998, 56)            0         ['re_lu_4[0][0]',             \n",
      " )                                                                   're_lu_7[0][0]',             \n",
      "                                                                     're_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 13998, 56)            0         ['concatenate_7[0][0]',       \n",
      " OpLambda)                                                           'concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 13996, 32)            5408      ['tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 13996, 32)            128       ['conv1d_9[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 13996, 8)             1288      ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 13996, 8)             32        ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 13996, 8)             0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenat  (None, 13996, 40)            0         ['batch_normalization_10[0][0]\n",
      " e)                                                                 ',                            \n",
      "                                                                     're_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 13996, 16)            3216      ['concatenate_10[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 13996, 16)            64        ['conv1d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 13996, 16)            0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenat  (None, 13996, 56)            0         ['batch_normalization_10[0][0]\n",
      " e)                                                                 ',                            \n",
      "                                                                     're_lu_9[0][0]',             \n",
      "                                                                     're_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPoolin  (None, 6998, 56)             0         ['concatenate_11[0][0]']      \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 6996, 32)             5408      ['max_pooling1d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 6996, 32)             128       ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 6996, 1)              0         ['batch_normalization_13[0][0]\n",
      "  (GlobalAveragePooling1D)                                          ']                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 6996)                 0         ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    13994     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " softmax (Softmax)           (None, 2)                    0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 51010 (199.26 KB)\n",
      "Trainable params: 50530 (197.38 KB)\n",
      "Non-trainable params: 480 (1.88 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Arsalan.tuh import TUHDataset\n",
    "dataset = TUHDataset('/home/athar/Files/TUKL_work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FlatMapDataset element_spec=(TensorSpec(shape=(21, 42001), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.bool, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = dataset.create_dataset(1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:38:09.458260: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 16:38:09.486480: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 16:38:09.486520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 16:38:09.487228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 16:38:09.492583: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 16:38:10.365065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-03-29 16:38:31.756320: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 112898752 bytes after encountering the first element of size 112898752 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    }
   ],
   "source": [
    "model.fit(train.repeat().padded_batch(32).take(10), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('scnet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('scnet.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
